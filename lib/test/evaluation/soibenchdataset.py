import os
import json
import numpy as np
import pandas as pd  # VideoCube ä¾èµ–
from lib.test.evaluation.data import Sequence, BaseDataset, SequenceList
# 1. æ›´æ–° Importsï¼šæ·»åŠ äº† load_str
from lib.test.utils.load_text import load_text, load_str
import pprint # å¦‚æœæ–‡ä»¶å¤´éƒ¨æ²¡å¯¼å…¥ï¼Œå¯ä»¥åœ¨è¿™é‡Œå¯¼å…¥ï¼Œæˆ–è€…æ”¾åˆ°æ–‡ä»¶æœ€ä¸Šé¢


################################################################################
# 
# ç« èŠ‚ 1: æ–°åŸºå‡†ç±» (SOIBenchmark)
#
################################################################################

class SOIBenchDataset(BaseDataset):
    """
    SOIBenchmark æ•°æ®é›†åŠ è½½å™¨ (ä¿®å¤ç‰ˆ)
    é€‚é…æ–‡ä»¶åæ ¼å¼: {sequence_name}_descriptions.jsonl
    """
    
    def __init__(self):
        super().__init__()
        
        # è¯·æ ¹æ®æ‚¨çš„å®é™…ç¯å¢ƒä¿®æ”¹ root_path
        root_path = "/home/member/data2/wyp/SUTrack/SOIBench/data/test" 
        
        self.dataset_loaders = {
            'lasot': {
                'loader': LaSOTDataset(), 
                'soi_dir': os.path.join(root_path, "lasot")
            },
            'videocube': {
                'loader': VideoCubeDataset(split='test', version='tiny'), 
                'soi_dir': os.path.join(root_path, "mgit")
            },
            'tnl2k': {
                'loader': TNL2kDataset(),
                'soi_dir': os.path.join(root_path, "tnl2k")
            },
        }

        self.soi_settings = {
            #
            "use_original_text": False,  # True=å¼ºåˆ¶ä½¿ç”¨åŸæ•°æ®é›†æ–‡æœ¬(æ¶ˆèç”¨); False=ä½¿ç”¨SOIæ ‡æ³¨

            # å†³å®šä½¿ç”¨å“ªäº›å±‚çº§
            "levels": [1, 2, 3, 4], 
            
            # æ¨¡å¼é€‰æ‹©: 'realtime' | 'fixed' | 'hierarchical'
            "persistence_mode": "realtime",  # "hierarchical", 
            
            # ä»…å½“ mode='fixed' æ—¶ç”Ÿæ•ˆ
            "fixed_duration": 30, 

            # ä»…å½“ mode='hierarchical' æ—¶ç”Ÿæ•ˆ
            # è¿™é‡Œå¯¹åº”æ‚¨çš„å‡è®¾ï¼šL1/L2(å¤–è§‚)æ›´æŒä¹…ï¼ŒL3(åŠ¨ä½œ)æ¬¡ä¹‹ï¼ŒL4(ç¯å¢ƒ)æœ€çŸ­
            "level_durations": {
                1: 999999,  # è¿‘ä¼¼æ°¸ä¹… (ç›´åˆ°è¢«æ–°æ ‡æ³¨è¦†ç›–)
                2: 999999,
                3: 60,      # åŠ¨ä½œæŒç»­çº¦ 2ç§’
                4: 30       # ç¯å¢ƒ/æ–¹ä½æŒç»­çº¦ 1ç§’
            }
        }

        # ================= [æ–°å¢] æ‰“å° SOI é…ç½®ä¿¡æ¯ =================
        self._print_config_info()
        # ==========================================================
        
        self.sequence_list = self._get_sequence_list()
    
    def _print_config_info(self):
        """æ‰“å°SOIé…ç½®ä¿¡æ¯,ç¡®ä¿æ¶ˆèå®éªŒé…ç½®æ¸…æ™°å¯è§"""
        print("\n" + "="*80)
        print(" " * 25 + "ğŸ” SOI BENCHMARK CONFIGURATION ğŸ”")
        print("="*80)
        
        # ç”Ÿæˆå¹¶æ‰“å°é…ç½®æ‘˜è¦
        summary = self._generate_config_summary()
        print(f"\nğŸ“‹ Configuration Summary:\n   {summary}\n")
        
        # æ‰“å°è¯¦ç»†é…ç½®
        print("ğŸ“ Detailed Settings:")
        print("-" * 80)
        for key, value in self.soi_settings.items():
            if key == 'level_durations' and isinstance(value, dict):
                print(f"  {key}:")
                for level, duration in sorted(value.items()):
                    print(f"    Level {level}: {duration} frames")
            else:
                print(f"  {key}: {value}")
        print("="*80)
        
    
    def _generate_config_summary(self):
        """ç”Ÿæˆç®€æ´çš„é…ç½®æ‘˜è¦"""
        if self.soi_settings.get('use_original_text', False):
            return "âš ï¸  ABLATION - Original Text Only (SOI Disabled)"
        
        mode = self.soi_settings.get('persistence_mode', 'fixed')
        levels = self.soi_settings.get('levels', [1, 2, 3, 4])
        
        if mode == 'realtime':
            duration_info = "Realtime åªåœ¨å½“å‰SOIå¸§æœ‰æ•ˆï¼Œå¦åˆ™å›é€€åŸæ–‡æœ¬"
        elif mode == 'fixed':
            fixed_dur = self.soi_settings.get('fixed_duration', 30)
            duration_info = f"Fixed duration ({fixed_dur} frames for all levels) SOIå¸§æ–‡æœ¬æŒç»­nå¸§"
        elif mode == 'hierarchical':
            level_durs = self.soi_settings.get('level_durations', {})
            dur_parts = [f"L{l}:{level_durs.get(l, 30)}f" for l in levels]
            duration_info = f"Hierarchical ({', '.join(dur_parts)}) SOIå¸§æ–‡æœ¬æ ¹æ®å±‚çº§è¿›è¡ŒæŒç»­ä½œç”¨"
        else:
            duration_info = f"Unknown mode: {mode}"
        
        return f"Levels: {levels} | Mode: {duration_info}"

    def get_sequence_list(self):
        return SequenceList(self.sequence_list)

    def _get_sequence_list(self):
        all_sequences = []
        print(f"Loading SOIBenchmark (JSONL version)...")

        for dset_name, config in self.dataset_loaders.items():
            loader_instance = config['loader']
            soi_dir_path = config['soi_dir']
            
            if not os.path.exists(soi_dir_path):
                print(f"  ! Warning: Directory not found: {soi_dir_path}")
                continue

            print(f"  > Scanning {dset_name} from {soi_dir_path} ...")
            
            # --- ä¿®æ”¹ç‚¹ 1: åªç­›é€‰ä»¥ _descriptions.jsonl ç»“å°¾çš„æ–‡ä»¶ ---
            file_list = [f for f in os.listdir(soi_dir_path) if f.endswith('_descriptions.jsonl')]
            
            valid_count = 0
            for filename in sorted(file_list):
                # --- ä¿®æ”¹ç‚¹ 2: æ­£ç¡®è§£æåºåˆ—å ---
                # ä¾‹å¦‚: "airplane-1_descriptions.jsonl" -> "airplane-1"
                seq_name = filename.replace('_descriptions.jsonl', '').replace('frame_', '')
                
                # try:
                # æ„é€ åºåˆ— (åŠ è½½ GT, å›¾ç‰‡è·¯å¾„ç­‰)
                sequence = loader_instance._construct_sequence(seq_name)
                
                # è¯»å–å¹¶æ³¨å…¥ SOI æ•°æ®
                full_jsonl_path = os.path.join(soi_dir_path, filename)
                processed_soi_data = self._read_and_process_soi(full_jsonl_path, sequence)
                sequence.soi_info = processed_soi_data
                # soi_data = self._read_jsonl(full_jsonl_path)
                
                sequence.dataset = 'soibench'
                sequence.original_dataset = dset_name

                all_sequences.append(sequence)
                valid_count += 1
                    
                # except Exception as e:
                #     # å¦‚æœ construct_sequence å¤±è´¥ï¼ˆä¾‹å¦‚åŸæ•°æ®é›†è·¯å¾„é‡Œæ²¡æœ‰è¿™ä¸ªåºåˆ—ï¼‰ï¼Œä¼šåœ¨è¿™é‡ŒæŠ¥é”™
                #     print(f"  ! Error loading sequence '{seq_name}' from {dset_name}: {e}")

            print(f"  > Loaded {valid_count} sequences from {dset_name}.")

        print(f"SOIBenchmark loaded. Total sequences: {len(all_sequences)}")
        return all_sequences

    def _read_jsonl(self, file_path):
        """
        è¯»å– jsonl æ–‡ä»¶ï¼Œè¿”å›åˆ—è¡¨
        """
        data_list = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    data = json.loads(line)
                    data_list.append(data)
                except json.JSONDecodeError as e:
                    print(f"    ! JSON Parse Error in {file_path}: {e}")
        return data_list

    def __len__(self):
        return len(self.sequence_list)
    
    def _read_and_process_soi(self, file_path, sequence):
        """
        è¯»å– SOI æ ‡æ³¨ (JSONLæ ¼å¼)ï¼Œæ”¯æŒï¼š
        1. åˆ†å±‚çº§æ–‡æœ¬æ‹¼æ¥ (Levels)
        2. åˆ†å±‚çº§æ—¶æ•ˆæ€§æ§åˆ¶ (Hierarchical Persistence)
        3. ç¼ºå¤±å€¼å›é€€ç­–ç•¥ (Fallback to Original Text)
        """

        # ==========================
        # 1. æå–åŸå§‹æ–‡æœ¬ (Fallback / Original)
        # ==========================
        # ä¸ç®¡å¼€ä¸å¼€æ¶ˆèï¼Œå…ˆæŠŠåŸå§‹æ–‡æœ¬æ‹¿å‡ºæ¥
        original_text = ""
        if hasattr(sequence, 'language_query') and sequence.language_query:
            original_text = str(sequence.language_query)
        elif hasattr(sequence, 'object_class') and sequence.object_class:
            original_text = str(sequence.object_class)
        
        if original_text:
            original_text = original_text.strip()
            if original_text:
                original_text = original_text[0].upper() + original_text[1:]
        
        total_frames = len(sequence.frames)

        # ==========================
        # [æ–°å¢] åŸå§‹æ–‡æœ¬æ¶ˆèé€»è¾‘
        # ==========================
        # å¦‚æœå¼€å¯äº† "ä½¿ç”¨åŸå§‹æ–‡æœ¬"ï¼Œåˆ™ç›´æ¥å¿½ç•¥ SOI JSONL æ–‡ä»¶
        # æ¯ä¸€å¸§éƒ½å¡«å…¥åŸå§‹æ–‡æœ¬ï¼Œä¸” need_update åªæœ‰ç¬¬0å¸§ä¸º True
        if self.soi_settings.get('use_original_text', False):
            
            soi_list = []
            for i in range(total_frames):
                # åªæœ‰ç¬¬0å¸§éœ€è¦æ›´æ–°ç‰¹å¾ï¼Œåé¢å…¨æ˜¯é™æ€æ–‡æœ¬ï¼Œä¸éœ€è¦æ›´æ–°
                need_upd = (i == 0) 
                soi_list.append({
                    'text': original_text,
                    'need_update': need_upd
                })
            return soi_list

        # ==========================
        # 2. å‡†å¤‡é…ç½®ä¸ Fallback æ–‡æœ¬
        # ==========================
        req_levels = self.soi_settings.get('levels', [1, 2, 3, 4])
        lang = self.soi_settings.get('language', 'en')
        lang_root_key = f"output-{lang}"
        mode = self.soi_settings.get('persistence_mode', 'fixed')
        
        # è·å–åŸå§‹æ•°æ®é›†çš„æ–‡æœ¬ä½œä¸ºâ€œä¿åº•â€
        fallback_text = ""
        if hasattr(sequence, 'language_query') and sequence.language_query:
            fallback_text = str(sequence.language_query)
        elif hasattr(sequence, 'object_class') and sequence.object_class:
            fallback_text = str(sequence.object_class)
        
        if fallback_text:
            fallback_text = fallback_text.strip()
            fallback_text = fallback_text[0].upper() + fallback_text[1:]

        # ==========================
        # 3. å‡†å¤‡æ—¶æ•ˆæ€§å‚æ•°
        # ==========================
        duration_map = {}
        if mode == 'realtime':
            for l in req_levels: duration_map[l] = 0
        elif mode == 'fixed':
            fixed = self.soi_settings.get('fixed_duration', 30)
            for l in req_levels: duration_map[l] = fixed
        elif mode == 'hierarchical':
            custom_durs = self.soi_settings.get('level_durations', {})
            for l in req_levels: 
                duration_map[l] = custom_durs.get(l, 30)

        # ==========================
        # 3. å»ºç«‹ç´¢å¼•æ˜ å°„ & è¯»å– JSONL
        # ==========================
        total_frames = len(sequence.frames)
        # name_to_idx = {os.path.basename(p): i for i, p in enumerate(sequence.frames)}
        
        sparse_anno = {}

        # é€è¡Œè¯»å– JSONL å¯¹äºå›¾ç‰‡åºåˆ—ï¼Œlasotæ˜¯ä»1å¼€å§‹ mgitæ˜¯ä»0å¼€å§‹å‘½å
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line: continue
                info = json.loads(line)
                idx = info.get('frame_idx')
                sparse_anno[idx] = info

        # ==========================
        # 4. æ ¸å¿ƒå¾ªç¯ï¼šçŠ¶æ€è¿½è¸ªä¸æ–‡æœ¬ç”Ÿæˆ (ç´§æ¥åœ¨ä¸Šé¢ä»£ç ä¹‹å)
        # ==========================
        soi_list = []
        last_frame_final_text = None
        # çŠ¶æ€è¿½è¸ªå™¨
        active_texts = {l: "" for l in req_levels}
        last_updates = {l: -99999 for l in req_levels}

        for i in range(total_frames):
            frame_anno = sparse_anno.get(i)
            
            # --- A. æ›´æ–°é˜¶æ®µ (Inject) ---
            if frame_anno and frame_anno.get('status') != 'skip':
                text_content = frame_anno.get(lang_root_key, {})
                for l in req_levels:
                    key = f"level{l}"
                    raw_txt = text_content.get(key, '').strip()
                    if raw_txt:
                        if l in [1, 4]:
                            processed = raw_txt[0].upper() + raw_txt[1:]
                        else:
                            processed = raw_txt[0].lower() + raw_txt[1:]
                        active_texts[l] = processed
                        last_updates[l] = i
                    else:
                        # å¦‚æœæ–°æ ‡æ³¨æ˜¾å¼ä¸ºç©ºï¼Œè¡¨ç¤ºè¯¥å±æ€§æ¶ˆå¤±
                        active_texts[l] = ""
                        last_updates[l] = i

            # --- B. æ£€æŸ¥è¿‡æœŸ (Expire) ---
            valid_parts = []
            for l in req_levels:
                txt = active_texts[l]
                if txt:
                    elapsed = i - last_updates[l]
                    if elapsed <= duration_map[l]:
                        valid_parts.append(txt)
                    else:
                        # è¿‡æœŸæ¸…ç†
                        active_texts[l] = ""

            # --- C. ç»„è£…æœ€ç»ˆæ–‡æœ¬ (Assemble & Fallback) ---
            final_text = " ".join(valid_parts)
            
            # å¦‚æœæ‹¼å‡ºæ¥çš„ SOI æ–‡æœ¬æ˜¯ç©ºçš„ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬å›é€€
            if not final_text.strip():
                final_text = fallback_text
                
            # --- Change Detection ---
            # å¦‚æœæ˜¯ç¬¬0å¸§ï¼Œæˆ–è€…æ–‡æœ¬è·Ÿä¸Šä¸€å¸§ä¸ä¸€æ ·ï¼Œåˆ™éœ€è¦æ›´æ–°
            need_update = (i == 0) or (final_text != last_frame_final_text)
            
            last_frame_final_text = final_text

            soi_list.append({'text': final_text, 'need_update': need_update})

        return soi_list


################################################################################
# 
# ç« èŠ‚ 2: å¤åˆ¶çš„åŸºç¡€æ•°æ®é›†ç±» (æŒ‰æ‚¨çš„è¦æ±‚)
# åœ¨è¿™é‡Œç²˜è´´ LaSOTDataset, VideoCubeDataset å’Œ TNL2kDataset çš„å®Œæ•´ä»£ç ã€‚
#
################################################################################

class LaSOTDataset(BaseDataset):
    """
    LaSOT test set consisting of 280 videos (see Protocol-II in the LaSOT paper)

    æ³¨æ„: è¿™æ˜¯ä¸€ä¸ªè¢« SOIBenchmark 'å€Ÿç”¨' çš„è¾…åŠ©ç±»ã€‚
    """
    def __init__(self):
        super().__init__()
        self.base_path = self.env_settings.lasot_path
        self.sequence_list = self._get_sequence_list_default() 
        self.clean_list = self.clean_seq_list()

    def clean_seq_list(self):
        clean_lst = []
        for i in range(len(self.sequence_list)):
            cls, _ = self.sequence_list[i].split('-')
            clean_lst.append(cls)
        return clean_lst

    def get_sequence_list(self):
        return SequenceList([self._construct_sequence(s) for s in self.sequence_list])

    def _construct_sequence(self, sequence_name):
        class_name = sequence_name.split('-')[0]
        anno_path = '{}/{}/{}/groundtruth.txt'.format(self.base_path, class_name, sequence_name)

        ground_truth_rect = load_text(str(anno_path), delimiter=',', dtype=np.float64)

        occlusion_label_path = '{}/{}/{}/full_occlusion.txt'.format(self.base_path, class_name, sequence_name)

        nlp_path = '{}/{}/{}/nlp.txt'.format(self.base_path, class_name, sequence_name)
        try:
            nlp_rect = load_text(str(nlp_path), delimiter=',', dtype=str)
            nlp_rect = str(nlp_rect)
        except:
            nlp_rect = '' # å®¹é”™

        full_occlusion = load_text(str(occlusion_label_path), delimiter=',', dtype=np.float64, backend='numpy')

        out_of_view_label_path = '{}/{}/{}/out_of_view.txt'.format(self.base_path, class_name, sequence_name)
        out_of_view = load_text(str(out_of_view_label_path), delimiter=',', dtype=np.float64, backend='numpy')

        target_visible = np.logical_and(full_occlusion == 0, out_of_view == 0)

        frames_path = '{}/{}/{}/img'.format(self.base_path, class_name, sequence_name)

        frames_list = ['{}/{:08d}.jpg'.format(frames_path, frame_number) for frame_number in range(1, ground_truth_rect.shape[0] + 1)]

        target_class = class_name
        return Sequence(sequence_name, frames_list, 'lasot', ground_truth_rect.reshape(-1, 4),
                        object_class=target_class, target_visible=target_visible, language_query=nlp_rect)

    def __len__(self):
        return len(self.sequence_list)

    def _get_sequence_list_default(self):
        # å®Œæ•´çš„ LaSOT åˆ—è¡¨...
        sequence_list = []
        return sequence_list

    def _get_sequence_list(self):
        return self._get_sequence_list_default()


class VideoCubeDataset(BaseDataset):
    """
    VideoCube test set
    
    æ³¨æ„: è¿™æ˜¯ä¸€ä¸ªè¢« SOIBenchmark 'å€Ÿç”¨' çš„è¾…åŠ©ç±»ã€‚
    """

    def __init__(self, split, version='tiny'):  # full
        super().__init__()

        self.split = split
        self.version = version

        json_path = os.path.join(os.path.split(os.path.realpath(__file__))[0], 'videocube.json')
        try:
            f = open(json_path, 'r', encoding='utf-8')
            self.infos = json.load(f)[self.version]
            f.close()
        except FileNotFoundError:
            print(f"é”™è¯¯: æ‰¾ä¸åˆ° {json_path}ã€‚")
            print("è¯·ç¡®ä¿ 'videocube.json' æ–‡ä»¶ä¸ 'soi_benchmark.py' ä½äºåŒä¸€ç›®å½• (lib/test/dataset/)ã€‚")
            raise
            
        self.sequence_list = self.infos[self.split]

        if split == 'test' or split == 'val':
            self.base_path = self.env_settings.videocube_path
        else:
            self.base_path = self.env_settings.videocube_path

    def get_sequence_list(self):
        return SequenceList([self._construct_sequence(s) for s in self.sequence_list])

    def _construct_sequence(self, sequence_name):
        anno_path = '{}/{}/{}/{}.txt'.format(self.base_path, 'attribute', 'groundtruth', sequence_name)

        ground_truth_rect = load_text(str(anno_path), delimiter=',', dtype=np.float64)

        frames_path = r'{}/{}/{}/{}/{}_{}'.format(self.base_path, 'data', self.split, sequence_name, 'frame', sequence_name)
        
        frame_list = [frame for frame in os.listdir(frames_path) if frame.endswith(".jpg")]
        frame_list.sort(key=lambda f: int(f[:-4]))
        frames_list = [os.path.join(frames_path, frame) for frame in frame_list]

        # (æ–°å¢) åŠ è½½ MGIT/VideoCube çš„ Story æ–‡æœ¬æè¿°
        # å‡è®¾ description æ–‡ä»¶å¤¹åœ¨æ•°æ®é›†æ ¹ç›®å½•ä¸‹ï¼Œæ–‡ä»¶åä¸åºåˆ—åä¸€è‡´ (e.g., 001.json)
        desc_path = '{}/{}/{}/{}.json'.format(self.base_path, 'attribute', 'description', sequence_name)
        
        story_text = "" 
        
        if os.path.exists(desc_path):
            try:
                with open(desc_path, 'r', encoding='utf-8') as f:
                    desc_data = json.load(f)
                
                # æå– story -> story_1 -> description
                # æ³¨æ„ï¼šæœ‰äº›æ–‡ä»¶å¯èƒ½æ²¡æœ‰ story_1 æˆ–è€…ç»“æ„ç¨æœ‰ä¸åŒï¼Œè¿™é‡Œåšä¸ªå®¹é”™
                if 'story' in desc_data and 'story_1' in desc_data['story']:
                    raw_text = desc_data['story']['story_1'].get('description', '')
                    if raw_text:
                        story_text = raw_text.strip()
                        # ç¡®ä¿é¦–å­—æ¯å¤§å†™
                        story_text = story_text[0].upper() + story_text[1:]
                else:
                    # å¦‚æœæ²¡æœ‰ storyï¼Œå°è¯•è¯»å– activity æˆ–å…¶ä»–å­—æ®µä½œä¸ºå¤‡é€‰ï¼ˆå¯é€‰ï¼‰
                    print(f"Warning: No 'story_1' found in {sequence_name}.json")
                    
            except Exception as e:
                print(f"Error loading description for {sequence_name}: {e}")
        else:
            # å¦‚æœæ‰¾ä¸åˆ°æè¿°æ–‡ä»¶ï¼Œå¯èƒ½æ‰“å°ä¸ªè­¦å‘Šæˆ–è€…ä¿æŒä¸ºç©º
            # print(f"Warning: Description file not found: {desc_path}")
            pass

        return Sequence(sequence_name, frames_list, 'videocube_{}'.format(self.split), ground_truth_rect.reshape(-1, 4), language_query=story_text)

    def __len__(self):
        return len(self.sequence_list)

    def _get_sequence_list_from_txt(self, split):
        path = r'{}/{}/{}_list.txt'.format(self.base_path, 'data', split)
        with open(path) as f:  # list.txt
            sequence_list = f.read().splitlines()
        return sequence_list


class TNL2kDataset(BaseDataset):
    """
    TNL2k test set

    æ³¨æ„: è¿™æ˜¯ä¸€ä¸ªè¢« SOIBenchmark 'å€Ÿç”¨' çš„è¾…åŠ©ç±»ã€‚
    """
    def __init__(self):
        super().__init__()
        self.base_path = self.env_settings.tnl2k_path
        self.sequence_list = self._get_sequence_list_default()

    def get_sequence_list(self):
        return SequenceList([self._construct_sequence(s) for s in self.sequence_list])

    def _construct_sequence(self, sequence_name):
        anno_path = '{}/{}/groundtruth.txt'.format(self.base_path, sequence_name)

        ground_truth_rect = load_text(str(anno_path), delimiter=',', dtype=np.float64)

        text_dsp_path = '{}/{}/language.txt'.format(self.base_path, sequence_name)
        text_dsp = load_str(text_dsp_path)

        frames_path = '{}/{}/imgs'.format(self.base_path, sequence_name)
        frames_list = [f for f in os.listdir(frames_path)]
        frames_list = sorted(frames_list)
        frames_list = ['{}/{}'.format(frames_path, frame_i) for frame_i in frames_list]

        return Sequence(sequence_name, frames_list, 'tnl2k', ground_truth_rect.reshape(-1, 4),
                        language_query=text_dsp)

    def __len__(self):
        return len(self.sequence_list)

    def _get_sequence_list_default(self):
        sequence_list = []
        for seq in os.listdir(self.base_path):
            if os.path.isdir(os.path.join(self.base_path, seq)):
                sequence_list.append(seq)
        return sequence_list
    
    def _get_sequence_list(self):
        return self._get_sequence_list_default()
