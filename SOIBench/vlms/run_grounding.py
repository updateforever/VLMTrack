# -*- coding: utf-8 -*-
"""
SOIBench/vlms/run_grounding.py
ç»Ÿä¸€çš„ Grounding æ¨ç†å…¥å£è„šæœ¬
æ”¯æŒå¤šç§ VLM æ¨¡å‹é€šè¿‡é€‚é…å™¨æ¥å…¥

ä½¿ç”¨æ–¹æ³•:
    # Qwen3VL API æ¨ç†
    python run_grounding.py --model qwen3vl --mode api
    
    # GLM-4.6V æœ¬åœ°æ¨ç†
    python run_grounding.py --model glm46v --mode local --model_path /path/to/model
    
    # DeepSeek-VL2 API æ¨ç†
    python run_grounding.py --model deepseekvl --mode api
    
    # æ·»åŠ æ–°æ¨¡å‹
    from model_adapters import register_adapter, ModelAdapter
    class MyVLMAdapter(ModelAdapter):
        ...
    register_adapter('myvlm', MyVLMAdapter)
"""

import argparse
import json
import os
from pathlib import Path
from typing import List, Dict
from tqdm import tqdm
from PIL import Image, ImageDraw, ImageFont, ImageColor

from model_adapters import get_adapter


# ============================================================================
# é€šç”¨è¾…åŠ©å‡½æ•°
# ============================================================================

_ADDITIONAL_COLORS = [name for (name, _) in ImageColor.colormap.items()]


def plot_bounding_boxes(im: Image.Image, bboxes: List[List[float]], save_path: str):
    """åœ¨å›¾ä¸Šç”» bbox"""
    if not bboxes:
        return
    
    img = im.copy()
    draw = ImageDraw.Draw(img)
    colors = ["red", "green", "blue", "yellow", "orange", "pink", "purple"] + _ADDITIONAL_COLORS
    
    try:
        font = ImageFont.truetype("NotoSansCJK-Regular.ttc", size=14)
    except Exception:
        font = ImageFont.load_default()
    
    for i, bbox in enumerate(bboxes):
        color = colors[i % len(colors)]
        x1, y1, x2, y2 = [int(round(v)) for v in bbox]
        draw.rectangle(((x1, y1), (x2, y2)), outline=color, width=3)
        draw.text((x1 + 4, y1 + 4), f"Pred-{i}", fill=color, font=font)
    
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    img.save(save_path)


def process_description_levels(output_en: Dict) -> List[str]:
    """å¤„ç†æè¿°æ–‡æœ¬çš„å››ä¸ªå±‚çº§ï¼Œæ·»åŠ åˆé€‚çš„æ ‡ç‚¹"""
    desc_parts = []
    for idx, k in enumerate(["level1", "level2", "level3", "level4"], 1):
        v = (output_en.get(k, "") or "").strip()
        if v:
            # ç§»é™¤æœ«å°¾çš„æ ‡ç‚¹ç¬¦å·
            v = v.rstrip('.,;:!?')
            
            # è½¬ä¸ºå°å†™
            v = v[0].lower() + v[1:] if len(v) > 0 else v
            
            # æ·»åŠ æ ‡ç‚¹
            if idx in [1, 2]:  # Level 1, 2: é€—å·
                v = v + ','
            else:  # Level 3, 4: å¥å·
                v = v + '.'
            
            desc_parts.append(v)
    
    return desc_parts


def load_and_fix_paths(jsonl_path: str, dataset_name: str, image_roots: Dict[str, str]) -> List[Dict]:
    """è¯»å–æè¿° jsonlï¼Œå¹¶æŠŠ image_path ä¿®å¤ä¸ºç»å¯¹è·¯å¾„"""
    image_root = image_roots.get(dataset_name)
    if not image_root:
        return []
    
    valid = []
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            item = json.loads(line)
            
            # æ³¨æ„ï¼šä¸è¦è·³è¿‡ skip å¸§ï¼
            # skip åªæ˜¯äººç±»æ ‡æ³¨æ—¶è·³è¿‡ï¼ŒVLM ç®—æ³•éœ€è¦å¯¹æ‰€æœ‰å¸§éƒ½è¿›è¡Œæ¨ç†
            
            # æå–æè¿°æ–‡æœ¬å¹¶æ·»åŠ åˆé€‚çš„æ ‡ç‚¹
            output_en = item.get("output-en", {}) or {}
            desc_parts = process_description_levels(output_en)
            
            if not desc_parts:
                # å¦‚æœæ²¡æœ‰æè¿°ï¼Œä½¿ç”¨é»˜è®¤æ–‡æœ¬
                print(f"  âš ï¸  WARNING: åºåˆ— {os.path.basename(jsonl_path)} çš„å¸§ {item.get('frame_idx')} ç¼ºå°‘æè¿°æ–‡æœ¬ï¼Œä½¿ç”¨é»˜è®¤ prompt")
                desc_parts = ["the target object."]
            
            # ä¿®å¤å›¾åƒè·¯å¾„
            rel = item.get("image_path", "")
            if not rel:
                continue
            if rel.startswith("/"):
                rel = rel[1:]
            
            # å°è¯•å¤šç§è·¯å¾„ç»„åˆæ–¹å¼
            possible_paths = [
                os.path.join(image_root, rel),
            ]
            
            # LaSOT ç‰¹æ®Šè·¯å¾„: éœ€è¦åœ¨ä¸­é—´æ’å…¥å¹´ä»½ç›®å½•
            if len(rel) > 10:
                possible_paths.append(os.path.join(image_root, rel[6:10], rel))
            
            # MGIT/TNL2K ç‰¹æ®Šè·¯å¾„
            if len(rel.split('/')) > 2:
                parts = rel.split('/')
                possible_paths.append(os.path.join(image_root, parts[1], 'imgs', parts[2][1:]))
                possible_paths.append(os.path.join(image_root, parts[1], 'imgs', parts[2]))
            
            abs_path = None
            for p in possible_paths:
                if p and os.path.exists(p):
                    abs_path = p
                    break
            
            if abs_path:
                valid.append({
                    "original_item": item,
                    "image_path": abs_path,
                    "desc_parts": desc_parts,
                    "dataset_name": dataset_name,
                    "frame_idx": item.get("frame_idx", "unknown"),
                })
    
    return valid


def _count_lines(path: str) -> int:
    """ç»Ÿè®¡æ–‡ä»¶è¡Œæ•°ï¼Œç”¨äºæ–­ç‚¹ç»­è·‘"""
    if not os.path.exists(path):
        return 0
    n = 0
    with open(path, "r", encoding="utf-8") as f:
        for _ in f:
            n += 1
    return n


def run_grounding_inference(
    adapter,
    engine,
    jsonl_dir: str,
    dataset_name: str,
    image_roots: Dict[str, str],
    output_dir: str,
    vis_dir: str = None,
    max_new_tokens: int = 512,
):
    """è¿è¡Œ Grounding æ¨ç†çš„ä¸»æµç¨‹"""
    # è·å–æ‰€æœ‰ JSONL æ–‡ä»¶å¹¶æ’åº
    jsonl_files = sorted([f for f in os.listdir(jsonl_dir) if f.endswith('_descriptions.jsonl')])
    if not jsonl_files:
        print(f"âš ï¸  ç›®å½•ä¸ºç©ºæˆ–æ²¡æœ‰ _descriptions.jsonl æ–‡ä»¶: {jsonl_dir}")
        return
    
    print(f"\nğŸ“‚ å¤„ç†æ•°æ®é›†: {dataset_name} ({len(jsonl_files)} ä¸ªåºåˆ—)")
    
    for jsonl_file in tqdm(jsonl_files, desc=f"å¤„ç† {dataset_name}", dynamic_ncols=True):
        seq_name = Path(jsonl_file).stem.replace("_descriptions", "").replace("_done", "")
        save_path = os.path.join(output_dir, f"{seq_name}_pred.jsonl")
        
        # æ–­ç‚¹ç»­è·‘: æ£€æŸ¥å·²å¤„ç†çš„è¡Œæ•°
        processed = _count_lines(save_path)
        jsonl_path = os.path.join(jsonl_dir, jsonl_file)
        samples = load_and_fix_paths(jsonl_path, dataset_name, image_roots)
        
        if processed >= len(samples):
            continue
        
        # ä»æ–­ç‚¹å¤„ç»§ç»­
        for s in samples[processed:]:
            img_path = s["image_path"]
            desc_parts = s["desc_parts"]
            
            # ä½¿ç”¨é€‚é…å™¨æ„é€  prompt
            prompt = adapter.build_prompt(desc_parts)
            
            # è°ƒç”¨æ¨ç†å¼•æ“
            try:
                raw_out = engine.chat(img_path, prompt, max_new_tokens=max_new_tokens)
            except Exception as e:
                print(f"  âŒ æ¨ç†å¤±è´¥: {e}")
                raw_out = ""
            
            # å¤„ç†ç©ºè¾“å‡º
            if not raw_out:
                record = s["original_item"].copy()
                record["model_response"] = raw_out
                record["parsed_bboxes"] = []
                with open(save_path, "a", encoding="utf-8") as f_out:
                    f_out.write(json.dumps(record, ensure_ascii=False) + "\n")
                continue
            
            # ä½¿ç”¨é€‚é…å™¨è§£æ bbox
            with Image.open(img_path) as img:
                w, h = img.size
                parsed = adapter.parse_response(raw_out, w, h)
            
            # ä¿å­˜ç»“æœ
            record = s["original_item"].copy()
            record["model_response"] = raw_out
            record["parsed_bboxes"] = parsed
            
            with open(save_path, "a", encoding="utf-8") as f_out:
                f_out.write(json.dumps(record, ensure_ascii=False) + "\n")
            
            # å¯é€‰: ä¿å­˜å¯è§†åŒ–
            if vis_dir and parsed:
                vis_path = os.path.join(vis_dir, f"{seq_name}_{s['frame_idx']}.jpg")
                with Image.open(img_path) as img:
                    plot_bounding_boxes(img, parsed, vis_path)


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="SOIBench Grounding æ¨ç†ç»Ÿä¸€å…¥å£",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # Qwen3VL API æ¨ç†
  python run_grounding.py --model qwen3vl --mode api
  
  # GLM-4.6V æœ¬åœ°æ¨ç†  
  python run_grounding.py --model glm46v --mode local
  
  # DeepSeek-VL2 API æ¨ç†
  python run_grounding.py --model deepseekvl --mode api
        """
    )
    
    # æ¨¡å‹é€‰æ‹©
    parser.add_argument("--model", type=str, required=True,
                        choices=["qwen3vl", "glm46v", "deepseekvl"],
                        help="æ¨¡å‹åç§°")
    
    # æ¨ç†æ¨¡å¼
    parser.add_argument("--mode", type=str, required=True,
                        choices=["local", "api"],
                        help="æ¨ç†æ¨¡å¼: local (æœ¬åœ°æ¨¡å‹) æˆ– api (API)")
    
    # æœ¬åœ°æ¨¡å‹å‚æ•°
    parser.add_argument("--model_path", type=str, default=None,
                        help="æœ¬åœ°æ¨¡å‹è·¯å¾„ (mode=local æ—¶ä½¿ç”¨ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„)")
    
    # API å‚æ•°
    parser.add_argument("--api_key", type=str, default=None,
                        help="API Key (é»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–)")
    parser.add_argument("--api_model_name", type=str, default=None,
                        help="API æ¨¡å‹åç§° (ä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤å€¼)")
    parser.add_argument("--api_base_url", type=str, default=None,
                        help="API Base URL (ä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤å€¼)")
    parser.add_argument("--api_temperature", type=float, default=0.1,
                        help="API æ¸©åº¦å‚æ•°")
    parser.add_argument("--api_max_tokens", type=int, default=512,
                        help="API æœ€å¤§ token æ•°")
    parser.add_argument("--api_retries", type=int, default=3,
                        help="API é‡è¯•æ¬¡æ•°")
    
    # æ•°æ®é›†å‚æ•°
    parser.add_argument("--lasot_jsonl", type=str,
                        default="/home/member/data2/wyp/SUTrack/SOIBench/data/test/lasot",
                        help="LaSOT JSONL æè¿°æ–‡ä»¶ç›®å½•")
    parser.add_argument("--lasot_root", type=str,
                        default="/home/member/data1/DATASETS_PUBLIC/LaSOT/LaSOTBenchmark",
                        help="LaSOT å›¾åƒæ ¹ç›®å½•")
    parser.add_argument("--mgit_jsonl", type=str,
                        default="/home/member/data2/wyp/SUTrack/SOIBench/data/test/mgit",
                        help="MGIT JSONL æè¿°æ–‡ä»¶ç›®å½•")
    parser.add_argument("--mgit_root", type=str,
                        default="/home/member/data1/DATASETS_PUBLIC/MGIT/VideoCube/data/test",
                        help="MGIT å›¾åƒæ ¹ç›®å½•")
    parser.add_argument("--tnl2k_jsonl", type=str,
                        default="/home/member/data2/wyp/SUTrack/SOIBench/data/test/tnl2k",
                        help="TNL2K JSONL æè¿°æ–‡ä»¶ç›®å½•")
    parser.add_argument("--tnl2k_root", type=str,
                        default="/home/member/data1/DATASETS_PUBLIC/TNL2K_test/TNL2K_test_subset",
                        help="TNL2K å›¾åƒæ ¹ç›®å½•")
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument("--output_root", type=str, default="./SOIBench/results",
                        help="è¾“å‡ºæ ¹ç›®å½•")
    parser.add_argument("--exp_tag", type=str, default=None,
                        help="å®éªŒæ ‡ç­¾ (ä¸æŒ‡å®šåˆ™ä½¿ç”¨æ¨¡å‹å)")
    parser.add_argument("--save_debug_vis", action="store_true",
                        help="æ˜¯å¦ä¿å­˜è°ƒè¯•å¯è§†åŒ–å›¾åƒ")
    
    # æ¨ç†å‚æ•°
    parser.add_argument("--max_new_tokens", type=int, default=512,
                        help="æœ€å¤§ç”Ÿæˆ token æ•°")
    
    args = parser.parse_args()
    
    # è·å–é€‚é…å™¨
    print(f"ğŸ”§ åŠ è½½æ¨¡å‹é€‚é…å™¨: {args.model}")
    adapter_class = get_adapter(args.model)
    adapter = adapter_class()
    
    # è®¾ç½®é»˜è®¤å€¼
    if args.mode == 'local' and not args.model_path:
        args.model_path = adapter.get_default_model_path()
        if not args.model_path:
            raise ValueError(f"æ¨¡å‹ {args.model} æ²¡æœ‰é»˜è®¤æœ¬åœ°è·¯å¾„ï¼Œè¯·ä½¿ç”¨ --model_path æŒ‡å®š")
        print(f"ğŸ“ ä½¿ç”¨é»˜è®¤æ¨¡å‹è·¯å¾„: {args.model_path}")
    
    if args.mode == 'api':
        if not args.api_model_name:
            args.api_model_name = adapter.get_default_api_model_name()
            print(f"ğŸ”¤ ä½¿ç”¨é»˜è®¤ API æ¨¡å‹å: {args.api_model_name}")
        if not args.api_base_url:
            args.api_base_url = adapter.get_default_api_base_url()
            print(f"ğŸŒ ä½¿ç”¨é»˜è®¤ API Base URL: {args.api_base_url}")
    
    if not args.exp_tag:
        args.exp_tag = args.model
    
    # åˆ›å»ºæ¨ç†å¼•æ“
    print(f"ğŸš€ åˆå§‹åŒ–æ¨ç†å¼•æ“ (mode={args.mode})")
    engine = adapter.create_engine(args)
    
    # æ„å»ºå›¾åƒæ ¹ç›®å½•å­—å…¸
    image_roots = {
        "lasot": args.lasot_root,
        "mgit": args.mgit_root,
        "tnl2k": args.tnl2k_root
    }
    
    # æ„å»ºä»»åŠ¡åˆ—è¡¨
    tasks = []
    if args.lasot_jsonl and os.path.exists(args.lasot_jsonl):
        tasks.append(("lasot", args.lasot_jsonl))
    if args.mgit_jsonl and os.path.exists(args.mgit_jsonl):
        tasks.append(("mgit", args.mgit_jsonl))
    if args.tnl2k_jsonl and os.path.exists(args.tnl2k_jsonl):
        tasks.append(("tnl2k", args.tnl2k_jsonl))
    
    if not tasks:
        print("âŒ æœªæŒ‡å®šä»»ä½•æœ‰æ•ˆçš„æ•°æ®ç›®å½•")
        return
    
    # å¤„ç†æ¯ä¸ªæ•°æ®é›†
    for dataset_name, jsonl_dir in tasks:
        out_dir = os.path.join(args.output_root, dataset_name, f"{args.mode}_{args.exp_tag}")
        os.makedirs(out_dir, exist_ok=True)
        
        vis_dir = None
        if args.save_debug_vis:
            vis_dir = os.path.join(out_dir, "vis_debug")
            os.makedirs(vis_dir, exist_ok=True)
        
        print(f"\n{'='*60}")
        print(f"å¤„ç†æ•°æ®é›†: {dataset_name}")
        print(f"JSONL ç›®å½•: {jsonl_dir}")
        print(f"å›¾åƒæ ¹ç›®å½•: {image_roots[dataset_name]}")
        print(f"è¾“å‡ºç›®å½•: {out_dir}")
        print(f"{'='*60}")
        
        # è¿è¡Œæ¨ç†
        run_grounding_inference(
            adapter=adapter,
            engine=engine,
            jsonl_dir=jsonl_dir,
            dataset_name=dataset_name,
            image_roots=image_roots,
            output_dir=out_dir,
            vis_dir=vis_dir,
            max_new_tokens=args.max_new_tokens,
        )
    
    print("\nâœ… å…¨éƒ¨ä»»åŠ¡å®Œæˆ")


if __name__ == "__main__":
    main()
